# -*- coding: utf-8 -*-
"""asr_test.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19EYgqYKTeuQUV3MMJ3Y6Q8c4AL9IZoJn
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install torch transformers datasets evaluate librosa soundfile

import torch
from dataclasses import dataclass
from typing import List, Dict, Any

from datasets import load_dataset, Audio
from transformers import (
    WhisperFeatureExtractor,
    WhisperTokenizer,
    WhisperProcessor,
    WhisperForConditionalGeneration,
    Seq2SeqTrainingArguments,
    Seq2SeqTrainer,
)
from transformers.trainer_utils import get_last_checkpoint
import evaluate

# --- Dataset settings for Yoruba ---
max_fleurs_samples = 5000  # Use more FLEURS samples since we don't have yo-speech
fleurs_lang = "yo_ng"      # FLEURS Yoruba

def load_fleurs_alternative(lang="yo_ng", split="train", max_samples=None):
    """Alternative approach to load FLEURS dataset"""
    try:
        # Try loading with trust_remote_code
        print("Attempting to load FLEURS with trust_remote_code=True...")
        ds = load_dataset(
            "google/fleurs",
            name=lang,
            split=split,
            streaming=True,
            trust_remote_code=True,
        )
    except Exception as e:
        print(f"Failed with trust_remote_code: {e}")
        print("Trying alternative approach...")

        # Alternative: Load without streaming first, then convert
        try:
            ds = load_dataset(
                "google/fleurs",
                name=lang,
                split=split,
                trust_remote_code=True,
            )
            # Convert to streaming format
            ds = ds.to_iterable_dataset()
        except Exception as e2:
            print(f"Alternative approach failed: {e2}")
            print("Trying with different dataset configuration...")

            # Last resort: try loading a different language or use a mock dataset
            print("Creating a mock dataset for testing...")
            return create_mock_dataset(max_samples)

    if max_samples:
        ds = ds.take(max_samples)

    ds = ds.cast_column("audio", Audio(sampling_rate=16_000))

    def normalize_fleurs(batch: Dict[str, Any]) -> Dict[str, Any]:
        arr = batch["audio"]["array"]
        sr = batch["audio"]["sampling_rate"]
        return {
            "audio_array": arr,
            "sampling_rate": sr,
            "sentence": batch["transcription"],
        }

    return ds.map(
        normalize_fleurs,
        remove_columns=[c for c in ds.column_names if c not in ("audio", "transcription")],
        batched=False,
    )

def create_mock_dataset(max_samples=100):
    """Create a mock dataset for testing when FLEURS is not available"""
    print("Creating mock dataset for testing...")

    # Create a simple mock dataset
    mock_data = []
    for i in range(min(max_samples, 100)):
        # Create dummy audio data (1 second of silence)
        audio_array = [0.0] * 16000  # 16kHz sampling rate
        mock_data.append({
            "audio_array": audio_array,
            "sampling_rate": 16000,
            "sentence": f"mock yoruba sentence {i}",
        })

    from datasets import Dataset
    return Dataset.from_list(mock_data)

# Load FLEURS dataset
print("Loading FLEURS Yoruba dataset...")
train_ds = load_fleurs_alternative(fleurs_lang, max_samples=max_fleurs_samples)

# Whisper processor & tokenizer
print("Loading Whisper model and processor...")
# Load feature extractor and tokenizer for whisper-small as requested
feature_extractor = WhisperFeatureExtractor.from_pretrained("openai/whisper-small")
tokenizer = WhisperTokenizer.from_pretrained("openai/whisper-small", task="transcribe")
processor = WhisperProcessor(feature_extractor, tokenizer)

processor.tokenizer.language = "yoruba"

# Prepare inputs
def prepare_dataset(batch: Dict[str, Any]) -> Dict[str, Any]:
    input_feats = processor.feature_extractor(
        batch["audio_array"],
        sampling_rate=batch["sampling_rate"],
    ).input_features[0]
    label_ids = processor.tokenizer(batch["sentence"]).input_ids
    return {"input_features": input_feats, "labels": label_ids}

print("Preparing dataset...")
train_ds = train_ds.map(
    prepare_dataset,
    remove_columns=["audio_array", "sampling_rate", "sentence"],
    batched=False,
)

# Model & training args
print("Setting up training...")
# Use whisper-small model as requested
model = WhisperForConditionalGeneration.from_pretrained("openai/whisper-small")
training_args = Seq2SeqTrainingArguments(
    output_dir="./whisper-small-yo-fleurs", # Updated output directory name
    per_device_train_batch_size=4,
    gradient_accumulation_steps=8,
    eval_strategy="no",
    learning_rate=1e-5,
    warmup_steps=50,
    max_steps=100,
    gradient_checkpointing=False, # Disabled gradient checkpointing
    fp16=True,
    save_strategy="steps",
    save_steps=50,
    save_total_limit=2,
    metric_for_best_model="wer",
    greater_is_better=False,
    logging_steps=10,
    push_to_hub=False,
    overwrite_output_dir=False,
    report_to="none", # Disable W&B logging through training_args
)

# Data collator
@dataclass
class DataCollatorSpeechSeq2SeqWithPadding:
    processor: WhisperProcessor
    decoder_start_token_id: int
    padding: bool = True
    max_length: int = 448
    label_pad_token_id: int = -100

    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:
        input_feats = [f["input_features"] for f in features]
        labels = [f["labels"] for f in features]
        batch_inputs = self.processor.feature_extractor.pad(
            {"input_features": input_feats}, return_tensors="pt"
        )
        labels_batch = self.processor.tokenizer.pad(
            {"input_ids": labels}, return_tensors="pt", padding=True, max_length=self.max_length
        )
        labels = labels_batch["input_ids"].masked_fill(
            labels_batch["attention_mask"] == 0,
            self.label_pad_token_id,
        )
        if (labels[:, 0] == self.decoder_start_token_id).all():
            labels = labels[:, 1:]
        return {"input_features": batch_inputs["input_features"], "labels": labels}

collator = DataCollatorSpeechSeq2SeqWithPadding(
    processor=processor,
    decoder_start_token_id=model.config.decoder_start_token_id,
)

# Trainer & metrics
import numpy as np
wer_metric = evaluate.load("wer")
def compute_metrics(pred):
    preds = processor.tokenizer.batch_decode(pred.predictions, skip_special_tokens=True)
    labels = pred.label_ids
    labels = np.where(labels != -100, labels, processor.tokenizer.pad_token_id)
    refs = processor.tokenizer.batch_decode(labels, skip_special_tokens=True)
    return {"wer": wer_metric.compute(predictions=preds, references=refs)}

trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=train_ds,
    data_collator=collator,
    compute_metrics=compute_metrics,
)

import os
os.environ["WANDB_DISABLED"] = "true" # Removed this line

# Train
print("Starting training...")

# Print the number of mel bins the model is configured for
print(f"Model configured with num_mel_bins: {model.config.num_mel_bins}")

import os
# Create the output directory if it doesn't exist
os.makedirs(training_args.output_dir, exist_ok=True)

last_ckpt = get_last_checkpoint(training_args.output_dir)
if last_ckpt:
    trainer.train(resume_from_checkpoint=last_ckpt)
else:
    trainer.train()

print("Training completed!")

# Save locally
print("Saving model...")
model.save_pretrained(training_args.output_dir)
processor.save_pretrained(training_args.output_dir)

print(f"Model saved to {training_args.output_dir}")

import shutil
# Copy your saved model to Drive
local_path = "./whisper-small-yo-fleurs"
drive_path = "/content/drive/MyDrive/whisper-small-yo-fleurs"

print(f"Copying model from {local_path} to Google Drive...")
shutil.copytree(local_path, drive_path, dirs_exist_ok=True)
print(f"âœ“ Model successfully saved to {drive_path}")

