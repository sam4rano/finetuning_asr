# -*- coding: utf-8 -*-
"""streaming_asr.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1E2Bq4tiLfUUdjud6I5LLdWKnrR4qH6Sl
"""

# ============================================================================
# STEP 1: INSTALL PACKAGES (no torchcodec needed)
# ============================================================================
!pip install -q transformers datasets evaluate jiwer accelerate librosa soundfile

# ============================================================================
# STEP 2: MOUNT GOOGLE DRIVE
# ============================================================================
from google.colab import drive
import os

drive.mount('/content/drive')

# Paths - UPDATE AS NEEDED
DRIVE_ROOT = "/content/drive/MyDrive"
PROJECT_DIR = f"{DRIVE_ROOT}/luhya_asr_project"
DATA_DIR = f"{PROJECT_DIR}/luhya_data"
OUTPUT_DIR = f"{PROJECT_DIR}/whisper-small-luhya"

os.makedirs(PROJECT_DIR, exist_ok=True)
os.makedirs(OUTPUT_DIR, exist_ok=True)

# ============================================================================
# STEP 3: IMPORT LIBRARIES
# ============================================================================
from datasets import Dataset
from transformers import (
    WhisperFeatureExtractor, WhisperTokenizer, WhisperProcessor,
    WhisperForConditionalGeneration, Seq2SeqTrainingArguments, Seq2SeqTrainer
)
import evaluate
import torch
from dataclasses import dataclass
from typing import List, Dict, Union
import pandas as pd
import json
import gc
from datetime import datetime
import librosa
import numpy as np
import random

torch.backends.cudnn.benchmark = True

# ============================================================================
# STEP 4: SYSTEM INFO
# ============================================================================
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Device: {device}")

# ============================================================================
# STEP 5: CONVERT LUHYA TSV TO WHISPER FORMAT (STREAMING - no audio loading)
# ============================================================================
def convert_luhya_tsv_to_whisper_format_streaming(tsv_path, audio_dir, max_samples=None):
    """
    Convert Luhya TSV format to Whisper-compatible HuggingFace Dataset
    Uses file paths only - no audio loading to avoid RAM issues
    """
    print(f"Converting Luhya data from: {tsv_path}")

    # Read TSV with error handling
    try:
        df = pd.read_csv(tsv_path, sep='\t', header=None, on_bad_lines='skip')
        print(f"✅ Loaded {len(df)} samples")
        print(f"   Columns: {len(df.columns)}")
    except Exception as e:
        print(f"❌ Error reading TSV: {e}")
        raise

    # Limit samples if specified
    if max_samples and len(df) > max_samples:
        df = df.head(max_samples)
        print(f"Limited to {max_samples} samples")

    # Show sample for verification
    if len(df) > 0:
        print(f"\nSample data (first row):")
        print(f"   Column 0 (path): {df.iloc[0][0]}")
        print(f"   Column 10 (transcription): {df.iloc[0][10][:100]}...")

    # Extract data (file paths only - no audio loading)
    audio_paths = []
    transcriptions = []
    speakers = []
    file_names = []

    skipped = 0
    for idx, row in df.iterrows():
        try:
            # Get filename from full path (column 0)
            full_path = str(row[0])
            filename = os.path.basename(full_path)

            # Build audio path in your Drive
            audio_path = os.path.join(audio_dir, filename)

            # Get transcription (column 10)
            if len(row) <= 10 or pd.isna(row[10]):
                print(f"⚠️  Row {idx}: Missing transcription, skipping")
                skipped += 1
                continue

            transcription = str(row[10]).strip()
            if not transcription or transcription == 'nan':
                print(f"⚠️  Row {idx}: Empty transcription, skipping")
                skipped += 1
                continue

            # Get speaker (column 1)
            speaker = str(row[1]) if len(row) > 1 and not pd.isna(row[1]) else "unknown"

            # Check if file exists
            if not os.path.exists(audio_path):
                print(f"⚠️  File not found: {audio_path}")
                skipped += 1
                continue

            # Store file path only (no audio loading)
            audio_paths.append(audio_path)
            transcriptions.append(transcription)
            speakers.append(speaker)
            file_names.append(filename)

            if (idx + 1) % 50 == 0:
                print(f"Processed {idx + 1}/{len(df)} samples...")

        except Exception as e:
            print(f"⚠️  Error processing row {idx}: {e}")
            skipped += 1
            continue

    print(f"✅ Successfully converted {len(audio_paths)} samples")
    print(f"⚠️  Skipped {skipped} samples due to errors")

    # Check if we have any data
    if len(audio_paths) == 0:
        raise ValueError("No valid audio samples found! Check your data paths and format.")

    # Create HuggingFace Dataset with file paths only
    dataset_dict = {
        "audio_path": audio_paths,
        "transcription": transcriptions,
        "speaker": speakers,
        "file_name": file_names
    }

    dataset = Dataset.from_dict(dataset_dict)
    return dataset

def split_dataset(dataset, train_split=0.85, val_split=0.10):
    """Split dataset into train/validation/test"""
    total = len(dataset)

    # Handle small datasets
    if total < 10:
        raise ValueError(f"Dataset too small ({total} samples). Need at least 10 samples.")

    train_size = max(int(total * train_split), 1)
    val_size = max(int(total * val_split), 1)

    # Ensure we have enough samples for each split
    if train_size + val_size >= total:
        train_size = int(total * 0.8)
        val_size = int(total * 0.1)

    # Shuffle indices with fixed seed for reproducibility
    indices = list(range(total))
    random.seed(42)
    random.shuffle(indices)

    train_indices = indices[:train_size]
    val_indices = indices[train_size:train_size + val_size]
    test_indices = indices[train_size + val_size:]

    train_dataset = dataset.select(train_indices)
    val_dataset = dataset.select(val_indices) if len(val_indices) > 0 else dataset.select([0])
    test_dataset = dataset.select(test_indices) if len(test_indices) > 0 else dataset.select([0])

    print(f"✅ Split: {len(train_dataset)} train, {len(val_dataset)} val, {len(test_dataset)} test")

    return train_dataset, val_dataset, test_dataset

# Convert your data (limit samples to avoid RAM issues)
TSV_FILE = f"{DATA_DIR}/recorder.tsv"
AUDIO_DIR = f"{DATA_DIR}"

print("Converting Luhya data to Whisper format...")
# Start with small sample to test
luhya_dataset = convert_luhya_tsv_to_whisper_format_streaming(TSV_FILE, AUDIO_DIR, max_samples=200)

# Split dataset
train_dataset, val_dataset, test_dataset = split_dataset(luhya_dataset)

# Save test set for later evaluation
test_dataset.save_to_disk(f"{OUTPUT_DIR}/test_dataset")
print(f"✅ Test dataset saved to: {OUTPUT_DIR}/test_dataset")

# ============================================================================
# STEP 6: MODEL SETUP
# ============================================================================
MODEL = "openai/whisper-small"
LANGUAGE_TOKEN = "sw"  # Use Swahili as proxy for Luhya

print(f"Setting up Whisper model: {MODEL}")
print(f"Language token: {LANGUAGE_TOKEN}")

# Load processor
feature_extractor = WhisperFeatureExtractor.from_pretrained(MODEL)
tokenizer = WhisperTokenizer.from_pretrained(MODEL, language=LANGUAGE_TOKEN, task="transcribe")
processor = WhisperProcessor(feature_extractor, tokenizer)

# Load model
model = WhisperForConditionalGeneration.from_pretrained(MODEL)
if torch.cuda.is_available():
    model = model.cuda()
    print("✅ Model moved to GPU")
else:
    print("⚠️  Running on CPU (training will be slow)")

print(f"✅ Model loaded: {MODEL}")
print(f"Model parameters: {model.num_parameters():,}")

# ============================================================================
# STEP 7: DATA PREPROCESSING (STREAMING - load audio on demand)
# ============================================================================
def prepare_dataset(batch):
    """Prepare dataset for training - load audio on demand to save RAM"""
    # Load audio from file path
    audio_path = batch["audio_path"]

    try:
        # Load audio with librosa (on demand)
        audio_array, sampling_rate = librosa.load(audio_path, sr=16000, mono=True)

        # Process features
        input_features = processor.feature_extractor(
            audio_array,
            sampling_rate=sampling_rate
        ).input_features[0]

        # Process labels - ensure it's a list of integers
        labels = processor.tokenizer(
            batch["transcription"],
            truncation=True,
            max_length=448,  # Fixed max length
            padding=False  # Don't pad here, let collator handle it
        ).input_ids

        # Ensure labels is a list of integers, not nested lists
        if isinstance(labels, list) and len(labels) > 0 and isinstance(labels[0], list):
            labels = labels[0]

        # Return new dict to avoid reference issues
        return {
            "input_features": input_features,
            "labels": labels
        }

    except Exception as e:
        print(f"Error loading audio {audio_path}: {e}")
        # Return dummy data for failed samples
        return {
            "input_features": np.zeros((80, 3000), dtype=np.float32),  # Dummy mel features
            "labels": [processor.tokenizer.pad_token_id]
        }

print("Preprocessing datasets...")
# Process in smaller batches to avoid memory issues
train_dataset = train_dataset.map(
    prepare_dataset,
    remove_columns=["audio_path", "transcription", "speaker", "file_name"],
    batched=False
)
val_dataset = val_dataset.map(
    prepare_dataset,
    remove_columns=["audio_path", "transcription", "speaker", "file_name"],
    batched=False
)
print("✅ Preprocessing completed!")

# Clear memory after preprocessing
gc.collect()
if torch.cuda.is_available():
    torch.cuda.empty_cache()

# ============================================================================
# STEP 8: DATA COLLATOR
# ============================================================================
from typing import Any, Dict, List, Union

# ============================================================================
# STEP 8: DATA COLLATOR
# ============================================================================
@dataclass
class DataCollatorSpeechSeq2SeqWithPadding:
    processor: WhisperProcessor
    decoder_start_token_id: int
    padding: bool = True
    max_length: int = 448
    label_pad_token_id: int = -100

    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:
        input_feats = [f["input_features"] for f in features]
        labels = [f["labels"] for f in features]
        batch_inputs = self.processor.feature_extractor.pad(
            {"input_features": input_feats}, return_tensors="pt"
        )
        labels_batch = self.processor.tokenizer.pad(
            {"input_ids": labels}, return_tensors="pt", padding=True, max_length=self.max_length
        )
        labels = labels_batch["input_ids"].masked_fill(
            labels_batch["attention_mask"] == 0,
            self.label_pad_token_id,
        )
        if (labels[:, 0] == self.decoder_start_token_id).all():
            labels = labels[:, 1:]
        return {"input_features": batch_inputs["input_features"], "labels": labels}

collator = DataCollatorSpeechSeq2SeqWithPadding(
    processor=processor,
    decoder_start_token_id=model.config.decoder_start_token_id,
)

# ============================================================================
# STEP 9: TRAINING ARGS (reduced for memory)
# ============================================================================
training_args = Seq2SeqTrainingArguments(
    output_dir=OUTPUT_DIR,
    per_device_train_batch_size=1,  # Minimal batch size
    gradient_accumulation_steps=32,  # Increased to maintain effective batch size
    eval_strategy="no",
    learning_rate=1e-5,
    warmup_steps=25,
    max_steps=50,  # Reduced for testing
    gradient_checkpointing=False,  # Disable to avoid backward pass issues
    fp16=True,
    save_strategy="steps",
    save_steps=25,
    save_total_limit=1,  # Keep only 1 checkpoint
    metric_for_best_model="wer",
    greater_is_better=False,
    logging_steps=5,
    push_to_hub=False,
    overwrite_output_dir=False,
    report_to="none",
    dataloader_pin_memory=False,  # Disable to save memory
    dataloader_num_workers=0,  # Disable multiprocessing to save memory
    remove_unused_columns=False,  # Prevent data issues
)

# ============================================================================
# STEP 10: METRICS & TRAINER
# ============================================================================
wer_metric = evaluate.load("wer")

def compute_metrics(pred):
    preds = processor.tokenizer.batch_decode(pred.predictions, skip_special_tokens=True)
    labels = pred.label_ids
    labels = np.where(labels != -100, labels, processor.tokenizer.pad_token_id)
    refs = processor.tokenizer.batch_decode(labels, skip_special_tokens=True)
    return {"wer": wer_metric.compute(predictions=preds, references=refs)}

trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    data_collator=collator,
    compute_metrics=compute_metrics,
)

# ============================================================================
# STEP 11: TRAIN
# ============================================================================
from transformers.trainer_utils import get_last_checkpoint
import os
os.environ["WANDB_DISABLED"] = "true"

print("Starting training...")
print(f"Model configured with num_mel_bins: {model.config.num_mel_bins}")

# Create the output directory if it doesn't exist
os.makedirs(training_args.output_dir, exist_ok=True)

# Clear memory before training
gc.collect()
if torch.cuda.is_available():
    torch.cuda.empty_cache()

# Attempt to get the last checkpoint. If it's invalid or doesn't exist,
# trainer.train() without resume_from_checkpoint will start from scratch.
last_ckpt = get_last_checkpoint(training_args.output_dir)

# Always call trainer.train() without resume_from_checkpoint in this case
# to avoid the ValueError when no valid checkpoint is found.
# The Trainer will start from scratch if last_ckpt is None or invalid for resuming.
trainer.train()


print("Training completed!")

# Save locally
print("Saving model...")
model.save_pretrained(training_args.output_dir)
processor.save_pretrained(training_args.output_dir)

print(f"Model saved to {training_args.output_dir}")

# ============================================================================
# STEP 12: EVALUATE ON TEST SET
# ============================================================================
print("Evaluating on test set...")

# Load test dataset from saved version
from datasets import load_from_disk
test_dataset = load_from_disk(f"{OUTPUT_DIR}/test_dataset")

# Process test dataset
test_dataset_processed = test_dataset.map(
    prepare_dataset,
    remove_columns=["audio_path", "transcription", "speaker", "file_name"],
    batched=False
)

# Clear memory before evaluation
gc.collect()
if torch.cuda.is_available():
    torch.cuda.empty_cache()

# Manual evaluation
print("Running manual evaluation...")
model.eval()

all_predictions = []
all_references = []

from torch.utils.data import DataLoader

test_dataloader = DataLoader(
    test_dataset_processed,
    batch_size=1,
    collate_fn=collator
)

with torch.no_grad():
    for batch in test_dataloader:
        input_features = batch["input_features"].to(device)
        labels = batch["labels"]

        # Generate predictions
        predicted_ids = model.generate(input_features, max_length=448)

        # Decode predictions
        predictions = processor.tokenizer.batch_decode(
            predicted_ids, skip_special_tokens=True
        )

        # Decode references
        labels = np.where(labels != -100, labels, processor.tokenizer.pad_token_id)
        references = processor.tokenizer.batch_decode(
            labels, skip_special_tokens=True
        )

        all_predictions.extend(predictions)
        all_references.extend(references)

# Compute WER
test_wer = wer_metric.compute(predictions=all_predictions, references=all_references)
test_results = {"eval_wer": test_wer}

print(f"Test WER: {test_results['eval_wer']:.4f}")

# Save results
with open(f"{OUTPUT_DIR}/test_results.json", "w") as f:
    json.dump(test_results, f, indent=2)

print(f"✅ Training completed! Model saved to: {OUTPUT_DIR}")
print(f"Test WER: {test_results['eval_wer']:.4f}")



"""for test data"""

# ============================================================================
# STEP 4: LOAD MODEL AND TEST DATA
# ============================================================================
print("Loading fine-tuned model...")
processor = WhisperProcessor.from_pretrained(OUTPUT_DIR)
model = WhisperForConditionalGeneration.from_pretrained(OUTPUT_DIR)

device = "cuda" if torch.cuda.is_available() else "cpu"
model = model.to(device)
print(f"Model loaded on {device}")

# Load test dataset
print("Loading test dataset...")
test_dataset = load_from_disk(f"{OUTPUT_DIR}/test_dataset")
print(f"Test dataset loaded: {len(test_dataset)} samples")

# ============================================================================
# STEP 5: PREPARE TEST DATA (SAME AS TRAINING)
# ============================================================================
def prepare_test_data(batch):
    """Prepare test data - same as training preprocessing"""
    audio_path = batch["audio_path"]

    try:
        # Load audio
        audio_array, sr = librosa.load(audio_path, sr=16000, mono=True)

        # Process features
        input_features = processor.feature_extractor(
            audio_array, sampling_rate=sr
        ).input_features[0]

        # Process labels
        labels = processor.tokenizer(
            batch["transcription"],
            truncation=True,
            max_length=448,
            padding=False
        ).input_ids

        # Ensure labels is a list of integers
        if isinstance(labels, list) and len(labels) > 0 and isinstance(labels[0], list):
            labels = labels[0]

        return {
            "input_features": input_features,
            "labels": labels,
            "transcription": batch["transcription"],  # Keep original for comparison
            "file_name": batch["file_name"]
        }

    except Exception as e:
        print(f"Error processing {audio_path}: {e}")
        return {
            "input_features": np.zeros((80, 3000), dtype=np.float32),
            "labels": [processor.tokenizer.pad_token_id],
            "transcription": "ERROR",
            "file_name": "ERROR"
        }

# Process test data
print("Preprocessing test data...")
test_dataset = test_dataset.map(prepare_test_data, remove_columns=["audio_path", "speaker"])

# ============================================================================
# STEP 6: TEST MODEL
# ============================================================================
def normalize_text(text):
    """Normalize text for better comparison"""
    import re
    # Convert to lowercase
    text = text.lower()
    # Remove extra whitespace
    text = re.sub(r'\s+', ' ', text)
    # Remove punctuation
    text = re.sub(r'[^\w\s]', '', text)
    # Strip leading/trailing whitespace
    text = text.strip()
    return text

def calculate_similarity(text1, text2):
    """Calculate similarity between two texts using character-level similarity"""
    from difflib import SequenceMatcher
    return SequenceMatcher(None, text1, text2).ratio()

def is_similar(ground_truth, prediction, threshold=0.85):
    """Check if two texts are similar enough to be considered a match"""
    # Normalize both texts
    gt_norm = normalize_text(ground_truth)
    pred_norm = normalize_text(prediction)

    # Check exact match first
    if gt_norm == pred_norm:
        return True

    # Check similarity
    similarity = calculate_similarity(gt_norm, pred_norm)
    return similarity >= threshold

def test_model_on_dataset(model, processor, test_dataset, max_samples=None):
    """Test model on the test dataset"""

    if max_samples:
        test_dataset = test_dataset.select(range(min(max_samples, len(test_dataset))))

    print(f"Testing on {len(test_dataset)} samples...")

    results = []
    predictions = []
    references = []

    for i, sample in enumerate(test_dataset):
        try:
            # Get input features
            input_features = torch.tensor(sample["input_features"]).unsqueeze(0).to(device)

            # Generate prediction
            with torch.no_grad():
                predicted_ids = model.generate(input_features)

            # Decode prediction
            prediction = processor.tokenizer.batch_decode(
                predicted_ids, skip_special_tokens=True
            )[0]

            # Store results
            ground_truth = sample["transcription"]
            file_name = sample["file_name"]

            # Calculate similarity
            similarity = calculate_similarity(normalize_text(ground_truth), normalize_text(prediction))
            is_match = is_similar(ground_truth, prediction)

            results.append({
                "file_name": file_name,
                "ground_truth": ground_truth,
                "prediction": prediction,
                "match": is_match,
                "similarity": similarity,
                "exact_match": ground_truth.lower().strip() == prediction.lower().strip()
            })

            predictions.append(prediction)
            references.append(ground_truth)

            # Print sample results
            print(f"\nSample {i+1}:")
            print(f"File: {file_name}")
            print(f"Ground Truth: {ground_truth}")
            print(f"Prediction: {prediction}")
            print(f"Similarity: {similarity:.3f}")
            print(f"Match (similar): {'✓' if is_match else '✗'}")
            print(f"Exact Match: {'✓' if ground_truth.lower().strip() == prediction.lower().strip() else '✗'}")

        except Exception as e:
            print(f"Error processing sample {i}: {e}")
            continue

    return results, predictions, references

# ============================================================================
# STEP 7: RUN TEST
# ============================================================================
print("Starting model testing...")

# Test on first 10 samples (adjust as needed)
test_results, predictions, references = test_model_on_dataset(
    model, processor, test_dataset, max_samples=10
)

# ============================================================================
# STEP 8: CALCULATE METRICS
# ============================================================================
print("\n" + "="*50)
print("TEST RESULTS")
print("="*50)

# Calculate accuracy (using similarity-based matching)
similarity_accuracy = sum([r["match"] for r in test_results]) / len(test_results)
exact_accuracy = sum([r["exact_match"] for r in test_results]) / len(test_results)
avg_similarity = sum([r["similarity"] for r in test_results]) / len(test_results)

print(f"Similarity-based Accuracy (85% threshold): {similarity_accuracy:.2%}")
print(f"Exact Match Accuracy: {exact_accuracy:.2%}")
print(f"Average Similarity: {avg_similarity:.3f}")

# Calculate WER
wer_metric = evaluate.load("wer")
wer_score = wer_metric.compute(predictions=predictions, references=references)
print(f"Word Error Rate (WER): {wer_score:.4f}")

# Calculate BLEU
bleu_metric = evaluate.load("bleu")
bleu_score = bleu_metric.compute(predictions=predictions, references=references)
print(f"BLEU Score: {bleu_score['bleu']:.4f}")

# Show similarity distribution
similarities = [r["similarity"] for r in test_results]
print(f"\nSimilarity Distribution:")
print(f"  High similarity (≥0.9): {sum(1 for s in similarities if s >= 0.9)}/{len(similarities)}")
print(f"  Good similarity (≥0.8): {sum(1 for s in similarities if s >= 0.8)}/{len(similarities)}")
print(f"  Fair similarity (≥0.7): {sum(1 for s in similarities if s >= 0.7)}/{len(similarities)}")
print(f"  Low similarity (<0.7): {sum(1 for s in similarities if s < 0.7)}/{len(similarities)}")

# ============================================================================
# STEP 9: SAVE RESULTS
# ============================================================================
results_summary = {
    "similarity_accuracy": similarity_accuracy,
    "exact_accuracy": exact_accuracy,
    "average_similarity": avg_similarity,
    "wer": wer_score,
    "bleu": bleu_score['bleu'],
    "total_samples": len(test_results),
    "similarity_matches": sum([r["match"] for r in test_results]),
    "exact_matches": sum([r["exact_match"] for r in test_results])
}

# Save detailed results
with open(f"{OUTPUT_DIR}/test_results_detailed.json", "w", encoding="utf-8") as f:
    json.dump(test_results, f, indent=2, ensure_ascii=False)

# Save summary
with open(f"{OUTPUT_DIR}/test_results_summary.json", "w", encoding="utf-8") as f:
    json.dump(results_summary, f, indent=2, ensure_ascii=False)

# Create CSV for easy viewing
df_results = pd.DataFrame(test_results)
df_results.to_csv(f"{OUTPUT_DIR}/test_results.csv", index=False)

print(f"\nResults saved to:")
print(f"- {OUTPUT_DIR}/test_results_detailed.json")
print(f"- {OUTPUT_DIR}/test_results_summary.json")
print(f"- {OUTPUT_DIR}/test_results.csv")

# ============================================================================
# STEP 10: DISPLAY SAMPLE RESULTS
# ============================================================================
print("\n" + "="*50)
print("SAMPLE RESULTS")
print("="*50)

for i, result in enumerate(test_results[:5]):  # Show first 5 results
    print(f"\nSample {i+1}:")
    print(f"File: {result['file_name']}")
    print(f"Ground Truth: {result['ground_truth']}")
    print(f"Prediction: {result['prediction']}")
    print(f"Match: {'✓' if result['match'] else '✗'}")

print(f"\n✅ Testing completed!")
print(f"Model: {OUTPUT_DIR}")
print(f"Test samples: {len(test_results)}")
print(f"Similarity-based Accuracy: {similarity_accuracy:.2%}")
print(f"Exact Match Accuracy: {exact_accuracy:.2%}")
print(f"Average Similarity: {avg_similarity:.3f}")
print(f"WER: {wer_score:.4f}")



